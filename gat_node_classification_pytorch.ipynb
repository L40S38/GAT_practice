{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAT node classification with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download Cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-08-05 12:09:37--  https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n",
      "Resolving linqs-data.soe.ucsc.edu (linqs-data.soe.ucsc.edu)... 128.114.47.74\n",
      "Connecting to linqs-data.soe.ucsc.edu (linqs-data.soe.ucsc.edu)|128.114.47.74|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 168052 (164K) [application/x-gzip]\n",
      "Saving to: 'cora.tgz'\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 30% 86.0K 1s\n",
      "    50K .......... .......... .......... .......... .......... 60%  141K 1s\n",
      "   100K .......... .......... .......... .......... .......... 91%  107K 0s\n",
      "   150K .......... ....                                       100% 35.2M=1.4s\n",
      "\n",
      "2023-08-05 12:09:39 (117 KB/s) - 'cora.tgz' saved [168052/168052]\n",
      "\n",
      "x cora/\n",
      "x cora/README\n",
      "x cora/cora.cites\n",
      "x cora/cora.content\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\" -O cora.tgz\n",
    "!tar -xvzf cora.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"cora\"\n",
    "\n",
    "citations = pd.read_csv(\n",
    "    os.path.join(data_dir, \"cora.cites\"),\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"target\", \"source\"],\n",
    ")\n",
    "\n",
    "papers = pd.read_csv(\n",
    "    os.path.join(data_dir, \"cora.content\"),\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"paper_id\"] + [f\"term_{idx}\" for idx in range(1433)] + [\"subject\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjectのリナンバリング \n",
    "class_idx = {name: id for id, name in enumerate(sorted(papers[\"subject\"].unique()))}\n",
    "# paperのリナンバリング\n",
    "paper_idx = {name: idx for idx, name in enumerate(sorted(papers[\"paper_id\"].unique()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers[\"paper_id\"] = papers[\"paper_id\"].apply(lambda name: paper_idx[name])\n",
    "citations[\"source\"] = citations[\"source\"].apply(lambda name: paper_idx[name])\n",
    "citations[\"target\"] = citations[\"target\"].apply(lambda name: paper_idx[name])\n",
    "papers[\"subject\"] = papers[\"subject\"].apply(lambda value: class_idx[value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(papers.iloc[:, 1:-1])\n",
    "edges = np.array(citations[[\"target\",\"source\"]])\n",
    "labels = np.array(papers[\"subject\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = features.shape[1]\n",
    "n_classes = len(np.unique(labels))\n",
    "n_nodes = features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#隣接行列の作成\n",
    "adj = np.zeros((n_nodes,n_nodes))\n",
    "for src,tgt in edges:\n",
    "    adj[src,tgt] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.from_numpy(features).float()\n",
    "adj = torch.from_numpy(adj).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データセットの分割\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.5, stratify=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットのクラス\n",
    "class CoraDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return idx, self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの作成\n",
    "train_dataset = CoraDataset(train_features, train_labels)\n",
    "test_dataset = CoraDataset(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaderの作成\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Graph Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Attention Layer\n",
    "class GraphAttention(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=0.6, alpha=0.2):\n",
    "        super(GraphAttention, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        torch.nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        torch.nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        h = torch.mm(input, self.W)\n",
    "        N = h.size()[0]\n",
    "\n",
    "        #a_input = torch.cat([h.repeat(1, N).view(N*N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2*self.out_features)\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N*N, -1), h.repeat(N, 1)], dim=1)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "\n",
    "        h_prime = torch.matmul(attention, h)\n",
    "        return h_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Graph Attention Layer\n",
    "class MultiHeadGraphAttention(nn.Module):\n",
    "    def __init__(self, in_features, out_features, n_heads, dropout=0.6, alpha=0.2):\n",
    "        super(MultiHeadGraphAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.heads = nn.ModuleList([GraphAttention(in_features, out_features, dropout=dropout, alpha=alpha) for _ in range(n_heads)])\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        head_outs = [head(input, adj) for head in self.heads]\n",
    "        return torch.mean(torch.stack(head_outs), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Attention Network\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, n_features, n_classes, n_hidden, n_heads, dropout=0.6, alpha=0.2):\n",
    "        super(GAT, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.attentions = nn.ModuleList([MultiHeadGraphAttention(n_features, n_hidden, n_heads, dropout=dropout, alpha=alpha) for _ in range(n_heads)])\n",
    "        self.out_att = GraphAttention(n_hidden*n_heads, n_classes, dropout=dropout, alpha=alpha)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        x = input\n",
    "        for att in self.attentions:\n",
    "            x = F.elu(att(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.out_att(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの定義\n",
    "n_hidden = 8\n",
    "n_heads = 8\n",
    "model = GAT(n_features, n_classes, n_hidden, n_heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの学習\n",
    "def train_model(model, optimizer, criterion, data_loader, adj):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for features, labels in data_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features, adj)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * features.size(0)\n",
    "    return total_loss / len(data_loader.dataset)\n",
    "\n",
    "# モデルの評価\n",
    "def evaluate_model(model, criterion, data_loader, adj):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in data_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            output = model(features, adj)\n",
    "            total_loss += criterion(output, labels).item() * features.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    return total_loss / len(data_loader.dataset), correct / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数と最適化手法の定義\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# 訓練に際して、可能であればGPU（cuda）を設定します。GPUが搭載されていない場合はCPUを使用します\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2708) must match the size of tensor b (64) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\NEC-PCuser\\OneDrive\\デスクトップ\\趣味開発用\\GAT_practice\\gat_node_classification_pytorch.ipynb Cell 27\u001b[0m in \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/NEC-PCuser/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/GAT_practice/gat_node_classification_pytorch.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/NEC-PCuser/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/GAT_practice/gat_node_classification_pytorch.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epochs):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/NEC-PCuser/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/GAT_practice/gat_node_classification_pytorch.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train_model(model, optimizer, criterion, train_loader, adj)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/NEC-PCuser/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/GAT_practice/gat_node_classification_pytorch.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     test_loss, test_accuracy \u001b[39m=\u001b[39m evaluate_model(model, criterion, test_loader, adj)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/NEC-PCuser/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/GAT_practice/gat_node_classification_pytorch.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mn_epochs\u001b[39m}\u001b[39;00m\u001b[39m, Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Test Loss: \u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Test Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtest_accuracy\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\NEC-PCuser\\OneDrive\\デスクトップ\\趣味開発用\\GAT_practice\\gat_node_classification_pytorch.ipynb Cell 27\u001b[0m in \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/NEC-PCuser/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/GAT_practice/gat_node_classification_pytorch.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m features, labels \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/NEC-PCuser/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/GAT_practice/gat_node_classification_pytorch.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/NEC-PCuser/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/GAT_practice/gat_node_classification_pytorch.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m output \u001b[39m=\u001b[39m model(features, adj)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/NEC-PCuser/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/GAT_practice/gat_node_classification_pytorch.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NEC-PCuser/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/GAT_practice/gat_node_classification_pytorch.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\NEC-PCuser\\anaconda3\\envs\\torch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\NEC-PCuser\\OneDrive\\デスクトップ\\趣味開発用\\GAT_practice\\gat_node_classification_pytorch.ipynb Cell 27\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NEC-PCuser/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/GAT_practice/gat_node_classification_pytorch.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NEC-PCuser/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/GAT_practice/gat_node_classification_pytorch.ipynb#X30sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m att \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattentions:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/NEC-PCuser/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/GAT_practice/gat_node_classification_pytorch.ipynb#X30sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39melu(att(x, adj))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NEC-PCuser/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/GAT_practice/gat_node_classification_pytorch.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NEC-PCuser/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/GAT_practice/gat_node_classification_pytorch.ipynb#X30sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_att(x, adj)\n",
      "File \u001b[1;32mc:\\Users\\NEC-PCuser\\anaconda3\\envs\\torch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\NEC-PCuser\\OneDrive\\デスクトップ\\趣味開発用\\GAT_practice\\gat_node_classification_pytorch.ipynb Cell 27\u001b[0m in \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/NEC-PCuser/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/GAT_practice/gat_node_classification_pytorch.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, adj):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/NEC-PCuser/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/GAT_practice/gat_node_classification_pytorch.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     head_outs \u001b[39m=\u001b[39m [head(\u001b[39minput\u001b[39m, adj) \u001b[39mfor\u001b[39;00m head \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NEC-PCuser/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/GAT_practice/gat_node_classification_pytorch.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mmean(torch\u001b[39m.\u001b[39mstack(head_outs), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\NEC-PCuser\\OneDrive\\デスクトップ\\趣味開発用\\GAT_practice\\gat_node_classification_pytorch.ipynb Cell 27\u001b[0m in \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/NEC-PCuser/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/GAT_practice/gat_node_classification_pytorch.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, adj):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/NEC-PCuser/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/GAT_practice/gat_node_classification_pytorch.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     head_outs \u001b[39m=\u001b[39m [head(\u001b[39minput\u001b[39;49m, adj) \u001b[39mfor\u001b[39;00m head \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NEC-PCuser/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/GAT_practice/gat_node_classification_pytorch.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mmean(torch\u001b[39m.\u001b[39mstack(head_outs), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\NEC-PCuser\\anaconda3\\envs\\torch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\NEC-PCuser\\OneDrive\\デスクトップ\\趣味開発用\\GAT_practice\\gat_node_classification_pytorch.ipynb Cell 27\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NEC-PCuser/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/GAT_practice/gat_node_classification_pytorch.ipynb#X30sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m e \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleakyrelu(torch\u001b[39m.\u001b[39mmatmul(a_input, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma)\u001b[39m.\u001b[39msqueeze(\u001b[39m2\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NEC-PCuser/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/GAT_practice/gat_node_classification_pytorch.ipynb#X30sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m zero_vec \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m9e15\u001b[39m\u001b[39m*\u001b[39mtorch\u001b[39m.\u001b[39mones_like(e)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/NEC-PCuser/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/GAT_practice/gat_node_classification_pytorch.ipynb#X30sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m attention \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mwhere(adj \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m, e, zero_vec)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NEC-PCuser/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/GAT_practice/gat_node_classification_pytorch.ipynb#X30sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m attention \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(attention, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NEC-PCuser/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/GAT_practice/gat_node_classification_pytorch.ipynb#X30sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m attention \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(attention, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2708) must match the size of tensor b (64) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# モデルの学習と評価\n",
    "n_epochs = 100\n",
    "model = model.to(device)\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_model(model, optimizer, criterion, train_loader, adj)\n",
    "    test_loss, test_accuracy = evaluate_model(model, criterion, test_loader, adj)\n",
    "    print(f\"Epoch {epoch + 1}/{n_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64]), torch.Size([64, 1433]))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices, input, _ = next(iter(test_loader))\n",
    "indices.shape,input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.empty(size=(n_features, n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.mm(input, W)\n",
    "N = h.size()[0]\n",
    "\n",
    "a_input = torch.cat([h.repeat(1, N).view(N*N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2*n_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 8])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 16])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([h.repeat(1, N).view(N*N, -1), h.repeat(N, 1)], dim=1).shape #これをLeaklyReluに通す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0],\n",
       "        [ 1],\n",
       "        [ 2],\n",
       "        ...,\n",
       "        [61],\n",
       "        [62],\n",
       "        [63]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.expand(N,N).reshape(N*N,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        ...,\n",
       "        [63],\n",
       "        [63],\n",
       "        [63]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.expand(N,N).permute(1,0).reshape(N*N,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0],\n",
       "        [ 1,  0],\n",
       "        [ 2,  0],\n",
       "        ...,\n",
       "        [61, 63],\n",
       "        [62, 63],\n",
       "        [63, 63]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_expand = torch.cat([indices.expand(N,N).reshape(N*N,-1),indices.expand(N,N).permute(1,0).reshape(N*N,-1)], dim=1)\n",
    "indices_expand"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
